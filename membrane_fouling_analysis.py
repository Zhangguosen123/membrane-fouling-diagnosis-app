# -*- coding: utf-8 -*-
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
from sklearn.metrics import r2_score
import time

# ==============================================================================
# <<-- Core Model Module
# ==============================================================================
USE_LOG_FIT = True
USE_HUBER = True
HUBER_DELTA = 0.002
GA_POP = 60
GA_GEN = 100
GA_MUT = 0.10
GA_ELITE = 2
RANDOM_SEED = 42
BOUNDS = [(0,1)]*4 + [(0.05,1)]*2  # Ks,Kc,Kb,Ki,a,b
EPS = 1e-12
EXP_FLOOR = -50.0

def stage1_model(params, t, J0):
    """Four-mechanism unified model"""
    Ks, Kc, Kb, Ki, a, b = params
    c1 = 10.0 * Ks * J0 / 2.0
    c2 = 10.0 * Kb
    c3 = 10.0 * Ki * J0
    c4 = 20.0 * Kc * J0**2

    base1 = np.maximum(1.0 + c1 * t, EPS)
    base3 = np.maximum(1.0 + c3 * t, EPS)
    base4 = np.maximum(1.0 + c4 * t, EPS)

    term1 = base1 ** (-2.0 * a)
    expo = np.maximum(-b * c2 * t, EXP_FLOOR)
    term2 = np.exp(expo)
    term3 = base3 ** (-(1.0 - b))
    term4 = base4 ** (-(1.0 - a) / 2.0)

    J_pred = J0 * term1 * term2 * term3 * term4
    return np.maximum(J_pred, EPS)

def huber_loss(residual, delta):
    """Huber loss function"""
    abs_r = np.abs(residual)
    quad = 0.5 * (abs_r ** 2)
    lin = delta * (abs_r - 0.5 * delta)
    return np.where(abs_r <= delta, quad, lin)

def objective(params, t, J_obs, J0):
    """Objective function"""
    J_pred = stage1_model(params, t, J0)
    mask = np.isfinite(J_obs) & np.isfinite(J_pred)
    if mask.sum() < 5:
        return 1e9
    y = J_obs[mask]; yhat = J_pred[mask]
    if USE_LOG_FIT:
        y = np.maximum(y, EPS)
        yhat = np.maximum(yhat, EPS)
        r = np.log(y) - np.log(yhat)
    else:
        r = y - yhat
    return np.mean(huber_loss(r, HUBER_DELTA)) if USE_HUBER else np.mean(r**2)

def genetic_algorithm(objective_fn, bounds, t, J_obs, J0):
    """Genetic algorithm optimization"""
    rng = np.random.default_rng(RANDOM_SEED)
    dim = len(bounds)
    pop = rng.random((GA_POP, dim))
    for i in range(dim):
        lo, hi = bounds[i]
        pop[:, i] = lo + pop[:, i] * (hi - lo)

    def fitness(ind):
        try:
            val = float(objective_fn(ind, t, J_obs, J0))
            return val if np.isfinite(val) else 1e9
        except Exception:
            return 1e9

    for _ in range(GA_GEN):
        scores = np.array([fitness(ind) for ind in pop])
        elite_idx = np.argsort(scores)[:GA_ELITE]
        new_pop = pop[elite_idx].copy()
        while len(new_pop) < GA_POP:
            idx1 = rng.integers(0, len(pop), size=3)
            p1 = pop[idx1[np.argmin(scores[idx1])]].copy()
            idx2 = rng.integers(0, len(pop), size=3)
            p2 = pop[idx2[np.argmin(scores[idx2])]].copy()
            cp = rng.integers(1, dim)
            child = np.concatenate([p1[:cp], p2[cp:]])
            for i in range(dim):
                if rng.random() < GA_MUT:
                    lo, hi = bounds[i]
                    child[i] += rng.normal(0, 0.1 * (hi - lo))
                    child[i] = np.clip(child[i], lo, hi)
            new_pop = np.vstack([new_pop, child])
        pop = new_pop

    scores = np.array([fitness(ind) for ind in pop])
    best = pop[np.argmin(scores)]
    return best

def fit_model(t, J_obs, J0):
    """Model fitting main function"""
    if len(t) < 5:
        return np.array([0.1,0.1,0.1,0.1,0.5,0.5])
    return genetic_algorithm(objective, BOUNDS, t, J_obs, J0)

def calculate_mechanism_contribution(params, t, J0):
    """Calculate contribution ratio of four fouling mechanisms"""
    Ks, Kc, Kb, Ki, a, b = params
    c1 = 10.0 * Ks * J0 / 2.0
    c2 = 10.0 * Kb
    c3 = 10.0 * Ki * J0
    c4 = 20.0 * Kc * J0**2

    s1 = - (2.0 * a) * c1 / (1.0 + c1 * t + EPS)
    s2 = - b * c2 * np.ones_like(t)
    s3 = - (1.0 - b) * c3 / (1.0 + c3 * t + EPS)
    s4 = - (1.0 - a) * c4 / (2.0 * (1.0 + c4 * t + EPS))
    
    Di = []
    for si in [s1, s2, s3, s4]:
        val = -np.trapz(si, t)
        Di.append(max(val, 0.0))
    Dsum = sum(Di) + EPS
    eta = np.array([d / Dsum for d in Di])
    return eta  # [Standard fouling, Complete fouling, Intermediate fouling, Cake fouling]

# ==============================================================================
# <<-- Utility Functions Module
# ==============================================================================

def read_csv_robust(path):
    """Robust CSV file reading (supports multiple encodings)"""
    for enc in ("utf-8-sig", "utf-8", "gbk", "latin1"):
        try:
            df = pd.read_csv(path, encoding=enc)
            return df, enc
        except Exception:
            continue
    raise RuntimeError(f"æ— æ³•è¯»å–æ–‡ä»¶: {path}")

def normalize_cols_to_standard(df):
    """Standardize column names (unify to "Time (s)" and "Flux")"""
    def norm_key(c):
        c = str(c).replace("\ufeff","").strip().replace("ï¼ˆ","(").replace("ï¼‰",")").replace(" ", "").lower()
        return c
    new_names = {}
    for c in df.columns:
        k = norm_key(c)
        if k in {"æ—¶é—´s","æ—¶é—´(s)","times","time(s)","time","t","æ—¶é—´"}:
            new_names[c] = "Time (s)"
        elif k in {"å®é™…é€šé‡","é€šé‡","flux","j"}:
            new_names[c] = "Flux"
    return df.rename(columns=new_names)

def clean_series(t, J):
    """Data cleaning (remove invalid values and tail outliers)"""
    mask = np.isfinite(t) & np.isfinite(J)
    t = t[mask]; J = J[mask]
    if len(t) > 0:
        k = max(int(round(len(t) * 0.99)), 5)
        t = t[:k]; J = J[:k]
    return t, J

def calculate_metrics(J_obs, J_pred):
    """Calculate fitting metrics (RÂ², NRMSE, MAPE)"""
    mask = np.isfinite(J_obs) & np.isfinite(J_pred)
    if mask.sum() == 0:
        return {"R2": np.nan, "NRMSE": np.nan, "MAPE": np.nan}
    y = J_obs[mask]; yhat = J_pred[mask]
    r2 = r2_score(y, yhat)
    rmse = np.sqrt(np.mean((y - yhat)**2))
    nrmse = rmse / (np.max(y) - np.min(y) + 1e-12) if (np.max(y) - np.min(y)) > 0 else np.nan
    mape_floor = max(1e-8, 0.05 * np.median(np.abs(y)))
    denom = np.maximum(np.abs(y), mape_floor)
    mape = np.mean(np.abs(y - yhat) / denom)
    return {
        "R2": round(r2, 3),
        "NRMSE": round(nrmse, 3) if np.isfinite(nrmse) else np.nan,
        "MAPE": round(mape, 3)
    }

def find_cleaning_time(t, J_pred, J0, acceptable_ratio=0.7):
    """
    Find the time point when flux drops to 70% of initial value (based on fitting curve)
    """
    acceptable_flux = J0 * acceptable_ratio
    # Find indices where fitting curve first drops below acceptable flux
    below_threshold_idx = np.where(J_pred <= acceptable_flux)[0]
    
    if len(below_threshold_idx) > 0:
        # Take the first point below threshold
        first_idx = below_threshold_idx[0]
        if first_idx == 0:
            # Initial point is already below threshold, return 0
            return 0.0, acceptable_flux, first_idx
        # Linear interpolation for more accurate time point
        t1, t2 = t[first_idx-1], t[first_idx]
        j1, j2 = J_pred[first_idx-1], J_pred[first_idx]
        # Interpolation formula: t = t1 + (t2-t1)*(acceptable_flux - j1)/(j2 - j1)
        cleaning_time = t1 + (t2 - t1) * (acceptable_flux - j1) / (j2 - j1)
        return cleaning_time, acceptable_flux, first_idx
    else:
        # Entire fitting curve is above threshold, return last point's time and flux
        return t[-1], J_pred[-1], len(t)-1

def recommend_cleaning_strategy(eta, stage="full"):
    """Recommend cleaning strategy based on dominant fouling mechanism"""
    # æ±¡æŸ“æœºåˆ¶åç§°æ”¹ä¸ºä¸­æ–‡
    mechanism_names = ["æ ‡å‡†æ±¡æŸ“ï¼ˆå­”é“æ”¶ç¼©ï¼‰", "å®Œå…¨æ±¡æŸ“ï¼ˆå­”é“å µå¡ï¼‰", 
                      "ä¸­é—´æ±¡æŸ“ï¼ˆå­”å£æ¡¥æ¥ï¼‰", "æ»¤é¥¼æ±¡æŸ“ï¼ˆè¡¨é¢æ²‰ç§¯ï¼‰"]
    dominant_idx = np.argmax(eta)
    dominant_mechanism = mechanism_names[dominant_idx]
    dominant_ratio = round(eta[dominant_idx] * 100, 1)
    
    # Adjust cleaning recommendations based on different stages
    if stage == "partial":  # 100%-70%é˜¶æ®µ
        if dominant_idx == 3:  # æ»¤é¥¼æ±¡æŸ“ä¸»å¯¼
            return f"ä¸»å¯¼æ±¡æŸ“ç±»å‹ï¼š{dominant_mechanism}ï¼ˆå æ¯”{dominant_ratio}%ï¼‰\
                   \næ¨èæ¸…æ´—ç­–ç•¥ï¼šåæ´—ï¼ˆå‹åŠ›0.08-0.1 MPaï¼Œæ—¶é•¿3-5åˆ†é’Ÿï¼‰\
                   \nä¼˜åŒ–å»ºè®®ï¼šè¯¥é˜¶æ®µä»¥è¡¨é¢æ»¤é¥¼å±‚ä¸ºä¸»ï¼Œåæ´—å¯æœ‰æ•ˆæ¢å¤é€šé‡"
        elif dominant_idx == 0 or dominant_idx == 1:  # æ ‡å‡†/å®Œå…¨æ±¡æŸ“ï¼ˆå†…éƒ¨æ±¡æŸ“ï¼‰
            return f"ä¸»å¯¼æ±¡æŸ“ç±»å‹ï¼š{dominant_mechanism}ï¼ˆå æ¯”{dominant_ratio}%ï¼‰\
                   \næ¨èæ¸…æ´—ç­–ç•¥ï¼šæŸ æª¬é…¸æº¶æ¶²æµ¸æ³¡ï¼ˆæµ“åº¦1-2%ï¼Œæ—¶é•¿10-15åˆ†é’Ÿï¼‰+ åæ´—\
                   \nä¼˜åŒ–å»ºè®®ï¼šæ—©æœŸå†…éƒ¨æ±¡æŸ“éœ€åŠæ—¶å¤„ç†ï¼Œé¿å…æ±¡æŸ“ç‰©æ¸—å…¥è†œå­”å†…éƒ¨"
        elif dominant_idx == 2:  # ä¸­é—´æ±¡æŸ“
            return f"ä¸»å¯¼æ±¡æŸ“ç±»å‹ï¼š{dominant_mechanism}ï¼ˆå æ¯”{dominant_ratio}%ï¼‰\
                   \næ¨èæ¸…æ´—ç­–ç•¥ï¼šå¼±ç¢±æ€§æ¸…æ´—ï¼ˆNaOHæº¶æ¶²ï¼ŒpH=9-10ï¼Œæ—¶é•¿10åˆ†é’Ÿï¼‰+ åæ´—\
                   \nä¼˜åŒ–å»ºè®®ï¼šæ§åˆ¶æ¸…æ´—å¼ºåº¦ï¼Œä¿æŠ¤è†œç»“æ„å®Œæ•´æ€§"
        else:
            return f"ä¸»å¯¼æ±¡æŸ“ç±»å‹ï¼šå¤šç§æœºåˆ¶å…±å­˜\
                   \næ¨èæ¸…æ´—ç­–ç•¥ï¼šæ¸©å’Œå¤åˆæ¸…æ´—ï¼ˆå…ˆæŸ æª¬é…¸åå¼±ç¢±ï¼‰\
                   \nä¼˜åŒ–å»ºè®®ï¼šé’ˆå¯¹å¤šç§æ±¡æŸ“ç±»å‹è¿›è¡ŒååŒå¤„ç†"
    else:  # å…¨æµç¨‹
        if dominant_idx == 3:  # æ»¤é¥¼æ±¡æŸ“ä¸»å¯¼
            return f"ä¸»å¯¼æ±¡æŸ“ç±»å‹ï¼š{dominant_mechanism}ï¼ˆå æ¯”{dominant_ratio}%ï¼‰\
                   \næ¨èæ¸…æ´—ç­–ç•¥ï¼šåæ´—ï¼ˆå‹åŠ›0.1 MPaï¼Œæ—¶é•¿5åˆ†é’Ÿï¼‰+ æ¬¡æ°¯é…¸é’ æ¸…æ´—ï¼ˆæµ“åº¦500 ppmï¼Œæ—¶é•¿10åˆ†é’Ÿï¼‰\
                   \nä¼˜åŒ–å»ºè®®ï¼šé€‚å½“é™ä½è¿è¡Œå‹åŠ›ï¼Œå‡å°‘æ»¤é¥¼å±‚å‹å®"
        elif dominant_idx == 0 or dominant_idx == 1:  # æ ‡å‡†/å®Œå…¨æ±¡æŸ“ï¼ˆå†…éƒ¨æ±¡æŸ“ï¼‰
            return f"ä¸»å¯¼æ±¡æŸ“ç±»å‹ï¼š{dominant_mechanism}ï¼ˆå æ¯”{dominant_ratio}%ï¼‰\
                   \næ¨èæ¸…æ´—ç­–ç•¥ï¼šæŸ æª¬é…¸æº¶æ¶²æµ¸æ³¡ï¼ˆæµ“åº¦2%ï¼Œæ—¶é•¿20åˆ†é’Ÿï¼‰+ åæ´—ï¼ˆå‹åŠ›0.15 MPaï¼Œæ—¶é•¿8åˆ†é’Ÿï¼‰\
                   \nä¼˜åŒ–å»ºè®®ï¼šé¢„å¤„ç†å»é™¤å°åˆ†å­æ±¡æŸ“ç‰©ï¼Œé™ä½å†…éƒ¨æ±¡æŸ“é£é™©"
        elif dominant_idx == 2:  # ä¸­é—´æ±¡æŸ“
            return f"ä¸»å¯¼æ±¡æŸ“ç±»å‹ï¼š{dominant_mechanism}ï¼ˆå æ¯”{dominant_ratio}%ï¼‰\
                   \næ¨èæ¸…æ´—ç­–ç•¥ï¼šç¢±æ€§æ¸…æ´—ï¼ˆNaOHæº¶æ¶²ï¼ŒpH=10ï¼Œæ—¶é•¿15åˆ†é’Ÿï¼‰+ åæ´—ï¼ˆå‹åŠ›0.12 MPaï¼Œæ—¶é•¿6åˆ†é’Ÿï¼‰\
                   \nä¼˜åŒ–å»ºè®®ï¼šæ§åˆ¶è¿›æ–™æµé€Ÿï¼Œå¢å¼ºå‰ªåˆ‡åŠ›ç ´é™¤å­”å£æ¡¥æ¥"
        else:
            return "æ±¡æŸ“ç±»å‹å‡è¡¡ï¼Œæ¨èå¤åˆæ¸…æ´—ï¼šåæ´— + æ¬¡æ°¯é…¸é’  + æŸ æª¬é…¸äº¤æ›¿æ¸…æ´—"

def load_validation_data(data_type, data_id):
    """Load validation data (unified file naming format)"""
    # äº‘ç«¯è·¯å¾„ï¼šå½“å‰ç›®å½•ï¼ˆä¸ä»£ç åŒå±‚çº§ï¼‰
    base_path = "."  
    
    # Updated file naming format: typeDataID.csv
    filename = f"{data_type}data{data_id}.csv"
    file_path = os.path.join(base_path, filename)
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}\nè¯·æ£€æŸ¥{filename}æ˜¯å¦å·²ä¸Šä¼ è‡³GitHubä»“åº“æ ¹ç›®å½•ã€‚")
    
    df, _ = read_csv_robust(file_path)
    df = normalize_cols_to_standard(df)
    
    # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
    if "Time (s)" not in df.columns or "Flux" not in df.columns:
        raise ValueError(f"æ–‡ä»¶{filename}ç¼ºå°‘å¿…è¦åˆ—ï¼éœ€è¦åŒ…å«'Time (s)'å’Œ'Flux'åˆ—ã€‚")
    
    t_raw = df["Time (s)"].values.astype(float)
    J_raw = df["Flux"].values.astype(float)
    t_clean, J_clean = clean_series(t_raw, J_raw)
    
    if len(J_clean) == 0:
        raise ValueError(f"{filename}æ¸…æ´—åæ— æœ‰æ•ˆé€šé‡æ•°æ®ï¼")
    
    J0 = J_clean[0]
    if J0 <= 0:
         raise ValueError(f"{filename}ä¸­çš„åˆå§‹é€šé‡å€¼ä¸ºé›¶æˆ–è´Ÿæ•°ï¼ˆ{J0}ï¼‰ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
        
    return t_clean, J_clean, J0, filename

# ==============================================================================
# <<-- Analysis Logic and GUI Interface
# ==============================================================================

def analyze_single_file(data_type, data_id):
    """Analyze single file and return result dictionary"""
    try:
        t_clean_sec, J_clean, J0, filename = load_validation_data(data_type, data_id)
        
        # 1. å…¨æµç¨‹åˆ†æ
        params_full = fit_model(t_clean_sec, J_clean, J0)
        J_pred_full = stage1_model(params_full, t_clean_sec - t_clean_sec[0], J0)
        metrics_full = calculate_metrics(J_clean, J_pred_full)
        eta_full = calculate_mechanism_contribution(params_full, t_clean_sec, J0)
        
        # 2. æŸ¥æ‰¾70%é€šé‡æ¸…æ´—ç‚¹
        cleaning_time_sec, cleaning_flux, cleaning_idx = find_cleaning_time(
            t_clean_sec, J_pred_full, J0, 0.7
        )
        
        # 3. 100%-70%é˜¶æ®µåˆ†æ
        # æˆªå–æ¸…æ´—ç‚¹å‰çš„æ•°æ®
        t_partial = t_clean_sec[:cleaning_idx+1]
        J_clean_partial = J_clean[:cleaning_idx+1]
        
        # é‡æ–°æ‹Ÿåˆæ¨¡å‹
        params_partial = fit_model(t_partial, J_clean_partial, J0)
        J_pred_partial = stage1_model(params_partial, t_partial - t_partial[0], J0)
        metrics_partial = calculate_metrics(J_clean_partial, J_pred_partial)
        eta_partial = calculate_mechanism_contribution(params_partial, t_partial, J0)
        
        # 4. ç”Ÿæˆæ¸…æ´—å»ºè®®
        cleaning_strategy_full = recommend_cleaning_strategy(eta_full, "full")
        cleaning_strategy_partial = recommend_cleaning_strategy(eta_partial, "partial")
        
        # æ±¡æŸ“æœºåˆ¶ç®€ç§°ï¼ˆä¸­æ–‡ï¼‰
        mechanism_names_short = ["æ ‡å‡†æ±¡æŸ“", "å®Œå…¨æ±¡æŸ“", "ä¸­é—´æ±¡æŸ“", "æ»¤é¥¼æ±¡æŸ“"]
        dominant_idx_full = np.argmax(eta_full)
        dominant_idx_partial = np.argmax(eta_partial)
        
        return {
            "success": True,
            "filename": filename,
            "data_type": data_type,
            "data_id": data_id,
            "J0": J0,
            # å…¨æµç¨‹åˆ†æç»“æœ
            "metrics_full": metrics_full,
            "eta_full": eta_full,
            "dominant_mechanism_full": mechanism_names_short[dominant_idx_full],
            "dominant_ratio_full": round(eta_full[dominant_idx_full] * 100, 1),
            "cleaning_strategy_full": cleaning_strategy_full,
            # 100%-70%é˜¶æ®µåˆ†æç»“æœ
            "metrics_partial": metrics_partial,
            "eta_partial": eta_partial,
            "dominant_mechanism_partial": mechanism_names_short[dominant_idx_partial],
            "dominant_ratio_partial": round(eta_partial[dominant_idx_partial] * 100, 1),
            "cleaning_strategy_partial": cleaning_strategy_partial,
            # æ¸…æ´—ç‚¹ä¿¡æ¯
            "cleaning_time": round(cleaning_time_sec, 2),
            "cleaning_flux": cleaning_flux,
            # æ•°æ®
            "t_clean_sec": t_clean_sec,
            "J_clean": J_clean,
            "J_pred_full": J_pred_full,
            "t_partial": t_partial,
            "J_pred_partial": J_pred_partial,
            "error": None
        }
    except Exception as e:
        filename = f"{data_type}data{data_id}.csv"
        return {
            "success": False,
            "filename": filename,
            "error": str(e)
        }

def main():
    # é€‚é…äº‘ç«¯å­—ä½“ï¼ˆæ”¯æŒä¸­æ–‡æ˜¾ç¤ºï¼‰
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']  # å¢åŠ é»‘ä½“æ”¯æŒä¸­æ–‡
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

    # é¡µé¢é…ç½®ï¼ˆä¸­æ–‡æ ‡é¢˜ï¼‰
    st.set_page_config(page_title="æ°´å¤„ç†è†œæ±¡æŸ“è¯Šæ–­ä¸æ¸…æ´—é¢„è­¦ç³»ç»Ÿ", page_icon="ğŸ’§", layout="wide")
    st.title("ğŸ’§ æ°´å¤„ç†è†œæ±¡æŸ“è¯Šæ–­ä¸æ¸…æ´—é¢„è­¦ç³»ç»Ÿ")
    
    # ä¾§è¾¹æ åˆ†ææ¨¡å¼é€‰æ‹©ï¼ˆä¸­æ–‡ï¼‰
    analysis_mode = st.sidebar.selectbox(
        "è¯·é€‰æ‹©åˆ†ææ¨¡å¼",
        ("å•æ–‡ä»¶åˆ†æ", "å…¨éƒ¨æ–‡ä»¶æ‰¹é‡åˆ†æ")
    )

    all_results = []
    
    if analysis_mode == "å•æ–‡ä»¶åˆ†æ":
        st.header("ğŸ“Š å•æ–‡ä»¶åˆ†æ")
        col1, col2 = st.columns(2)
        with col1:
            # æ±¡æŸ“ç‰©ç±»å‹ä¸‹æ‹‰æ¡†ï¼ˆä¸­æ–‡ï¼‰
            data_type = st.selectbox("æ±¡æŸ“ç‰©ç±»å‹", ["BSA", "HA", "SA"])
        with col2:
            # æ•°æ®IDä¸‹æ‹‰æ¡†ï¼ˆä¸­æ–‡ï¼‰
            data_id = st.selectbox("æ•°æ®ID", [1])
        
        # åˆ†ææŒ‰é’®ï¼ˆä¸­æ–‡ï¼‰
        if st.button("å¼€å§‹åˆ†æ"):
            with st.spinner(f"æ­£åœ¨åˆ†æ {data_type}data{data_id}.csv ..."):
                result = analyze_single_file(data_type, data_id)
                all_results.append(result)
        
    else:
        st.header("ğŸ“Š å…¨éƒ¨æ–‡ä»¶æ‰¹é‡åˆ†æ")
        st.warning("âš ï¸ æ‰¹é‡åˆ†æå°†å¤„ç†å…¨éƒ¨3ä¸ªæ–‡ä»¶ï¼ˆBSAdata1.csvã€HAdata1.csvã€SAdata1.csvï¼‰ã€‚")
        
        # æ‰¹é‡åˆ†ææŒ‰é’®ï¼ˆä¸­æ–‡ï¼‰
        if st.button("å¼€å§‹æ‰¹é‡åˆ†æ"):
            files_to_process = [("BSA", 1), ("HA", 1), ("SA", 1)]
            total_files = len(files_to_process)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            for i, (data_type, data_id) in enumerate(files_to_process):
                progress = (i + 1) / total_files
                status_text.text(f"æ­£åœ¨åˆ†æï¼ˆ{i+1}/{total_files}ï¼‰ï¼š{data_type}data{data_id}.csv")
                result = analyze_single_file(data_type, data_id)
                all_results.append(result)
                progress_bar.progress(progress)
                time.sleep(0.1)
            
            progress_bar.empty()
            status_text.text("âœ… æ‰¹é‡åˆ†æå®Œæˆï¼")

    if all_results:
        st.markdown("---")
        st.header("ğŸ“ˆ åˆ†æç»“æœæ±‡æ€»")
        
        summary_data = []
        for res in all_results:
            if res["success"]:
                cleaning_status = f"{res['cleaning_time']} ç§’" if res['cleaning_time'] > 0 else "ç«‹å³æ¸…æ´—"
                summary_data.append({
                    "æ–‡ä»¶å": res["filename"],
                    "ç±»å‹": res["data_type"],
                    "åˆå§‹é€šé‡ (LMS)": f'{res["J0"]:.2f}',
                    "æ¨èæ¸…æ´—æ—¶é—´": cleaning_status,
                    "å…¨æµç¨‹ä¸»å¯¼æ±¡æŸ“ç±»å‹": f'{res["dominant_mechanism_full"]}ï¼ˆ{res["dominant_ratio_full"]}%ï¼‰',
                    "100%-70%é˜¶æ®µä¸»å¯¼æ±¡æŸ“ç±»å‹": f'{res["dominant_mechanism_partial"]}ï¼ˆ{res["dominant_ratio_partial"]}%ï¼‰',
                    "çŠ¶æ€": "æˆåŠŸ"
                })
            else:
                summary_data.append({
                    "æ–‡ä»¶å": res["filename"],
                    "ç±»å‹": "æ— ",
                    "åˆå§‹é€šé‡ (LMS)": "æ— ",
                    "æ¨èæ¸…æ´—æ—¶é—´": "æ— ",
                    "å…¨æµç¨‹ä¸»å¯¼æ±¡æŸ“ç±»å‹": "æ— ",
                    "100%-70%é˜¶æ®µä¸»å¯¼æ±¡æŸ“ç±»å‹": "æ— ",
                    "çŠ¶æ€": f'å¤±è´¥ï¼š{res["error"]}'
                })
        
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df, use_container_width=True)

        if summary_data:
            csv = summary_df.to_csv(index=False, encoding='utf-8-sig')
            # ä¸‹è½½æŒ‰é’®ï¼ˆä¸­æ–‡ï¼‰
            st.download_button(
                label="ğŸ’¾ ä¸‹è½½æ±‡æ€»ç»“æœï¼ˆCSVï¼‰",
                data=csv,
                file_name="è†œæ±¡æŸ“åˆ†ææ±‡æ€»ç»“æœ.csv",
                mime="text/csv",
            )

        st.markdown("---")
        st.subheader("ğŸ” è¯¦ç»†åˆ†ææŠ¥å‘Š")
        
        for res in all_results:
            if res["success"]:
                with st.expander(f"ğŸ“„ {res['filename']} è¯¦ç»†æŠ¥å‘Š", expanded=False):
                    # ç¬¬ä¸€è¡Œï¼šåŸºç¡€ä¿¡æ¯å’Œæ‹Ÿåˆç»“æœ
                    col1, col2 = st.columns(2)
                    with col1:
                        st.subheader("åŸºç¡€ä¿¡æ¯")
                        st.write(f"åˆå§‹é€šé‡ï¼š{res['J0']:.2f} LMS")
                        st.write(f"æ¨èæ¸…æ´—æ—¶é—´ï¼š{res['cleaning_time']:.1f} ç§’")
                        st.write(f"æ¸…æ´—ç‚¹é€šé‡ï¼š{res['cleaning_flux']:.2f} LMSï¼ˆåˆå§‹é€šé‡çš„70%ï¼‰")
                    
                    with col2:
                        st.subheader("å…¨æµç¨‹æ‹Ÿåˆç»“æœ")
                        st.write(f"å†³å®šç³»æ•°RÂ²ï¼š{res['metrics_full']['R2']:.3f}")
                        st.write(f"å½’ä¸€åŒ–å‡æ–¹æ ¹è¯¯å·®NRMSEï¼š{res['metrics_full']['NRMSE']:.3f}")
                        st.write(f"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®MAPEï¼š{res['metrics_full']['MAPE']:.3f}")
                        st.write("**100%-70%é˜¶æ®µæ‹Ÿåˆç»“æœ**")
                        st.write(f"å½’ä¸€åŒ–å‡æ–¹æ ¹è¯¯å·®NRMSEï¼š{res['metrics_partial']['NRMSE']:.3f}")
                        st.write(f"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®MAPEï¼š{res['metrics_partial']['MAPE']:.3f}")
                    
                    # ç¬¬äºŒè¡Œï¼šæ±¡æŸ“æœºåˆ¶åˆ†æå¯¹æ¯”
                    st.markdown("---")
                    st.subheader("æ±¡æŸ“æœºåˆ¶åˆ†æå¯¹æ¯”")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**å…¨æµç¨‹æ±¡æŸ“æœºåˆ¶å æ¯”**")
                        sizes_full = [max(round(e*100, 1), 0.0) for e in res["eta_full"]]
                        labels_full = ["æ ‡å‡†æ±¡æŸ“", "å®Œå…¨æ±¡æŸ“", "ä¸­é—´æ±¡æŸ“", "æ»¤é¥¼æ±¡æŸ“"]
                        # è¿‡æ»¤æ‰å æ¯”ä¸º0çš„ç±»åˆ«
                        filtered_sizes_full = []
                        filtered_labels_full = []
                        for s, l in zip(sizes_full, labels_full):
                            if s > 0:
                                filtered_sizes_full.append(s)
                                filtered_labels_full.append(l)
                        fig1, ax1 = plt.subplots(figsize=(5, 4))
                        ax1.pie(filtered_sizes_full, labels=filtered_labels_full, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'])
                        ax1.axis('equal')
                        st.pyplot(fig1)
                        st.info(f"**å…¨æµç¨‹æ¸…æ´—å»ºè®®**\n\n{res['cleaning_strategy_full']}")
                    
                    with col2:
                        st.write("**100%-70%é˜¶æ®µæ±¡æŸ“æœºåˆ¶å æ¯”**")
                        sizes_partial = [max(round(e*100, 1), 0.0) for e in res["eta_partial"]]
                        labels_partial = ["æ ‡å‡†æ±¡æŸ“", "å®Œå…¨æ±¡æŸ“", "ä¸­é—´æ±¡æŸ“", "æ»¤é¥¼æ±¡æŸ“"]
                        # è¿‡æ»¤æ‰å æ¯”ä¸º0çš„ç±»åˆ«
                        filtered_sizes_partial = []
                        filtered_labels_partial = []
                        for s, l in zip(sizes_partial, labels_partial):
                            if s > 0:
                                filtered_sizes_partial.append(s)
                                filtered_labels_partial.append(l)
                        fig2, ax2 = plt.subplots(figsize=(5, 4))
                        ax2.pie(filtered_sizes_partial, labels=filtered_labels_partial, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99','#ffcc99'])
                        ax2.axis('equal')
                        st.pyplot(fig2)
                        st.info(f"**100%-70%é˜¶æ®µæ¸…æ´—å»ºè®®**\n\n{res['cleaning_strategy_partial']}")
                    
                    # ç¬¬ä¸‰è¡Œï¼šé€šé‡è¡°å‡æ‹Ÿåˆæ›²çº¿åˆ†æ
                    st.markdown("---")
                    st.subheader("é€šé‡è¡°å‡æ‹Ÿåˆæ›²çº¿åˆ†æ")
                    fig3, ax3 = plt.subplots(figsize=(10, 6))
                    
                    # ç»˜åˆ¶å…¨æµç¨‹æ•°æ®
                    ax3.plot(res["t_clean_sec"], res["J_clean"], 'o', ms=3, label='å®é™…è§‚æµ‹å€¼', color='gray', alpha=0.6)
                    ax3.plot(res["t_clean_sec"], res["J_pred_full"], '-', lw=2, label='å…¨æµç¨‹æ‹Ÿåˆæ›²çº¿', color='orange', alpha=0.8)
                    
                    # ç»˜åˆ¶100%-70%é˜¶æ®µæ•°æ®ï¼ˆåŠ ç²—ï¼‰
                    ax3.plot(res["t_partial"], res["J_pred_partial"], '-', lw=3, label='100%-70%é˜¶æ®µæ‹Ÿåˆæ›²çº¿', color='green')
                    
                    # ç»˜åˆ¶70%é€šé‡é˜ˆå€¼çº¿
                    acceptable_flux = res["J0"] * 0.7
                    ax3.axhline(y=acceptable_flux, color='red', linestyle=':', label='70%åˆå§‹é€šé‡ï¼ˆæ¨èæ¸…æ´—ç‚¹ï¼‰')
                    
                    # ç»˜åˆ¶æ¨èæ¸…æ´—ç‚¹å’Œè¿æ¥çº¿
                    cleaning_time = res["cleaning_time"]
                    ax3.scatter(cleaning_time, acceptable_flux, color='red', s=80, zorder=5, label='æ¨èæ¸…æ´—æ—¶é—´ç‚¹')
                    ax3.axvline(x=cleaning_time, color='red', linestyle='--', alpha=0.7)
                    ax3.text(cleaning_time, 0, f'æ¨èæ¸…æ´—æ—¶é—´ï¼š{cleaning_time:.1f}ç§’', 
                            horizontalalignment='center', verticalalignment='bottom', 
                            color='red', fontsize=10, fontweight='bold')
                    
                    # é«˜äº®100%-70%é˜¶æ®µåŒºåŸŸ
                    ax3.axvspan(0, cleaning_time, alpha=0.1, color='green', label='100%-70%æ¨èè¿è¡ŒåŒºé—´')
                    
                    ax3.set_xlabel('æ—¶é—´ï¼ˆç§’ï¼‰')
                    ax3.set_ylabel('é€šé‡ï¼ˆLMSï¼‰')
                    ax3.legend(loc='best')
                    ax3.grid(alpha=0.3)
                    # è®¾ç½®yè½´èŒƒå›´
                    y_min = min(res["J_clean"].min(), acceptable_flux) * 0.8
                    y_max = res["J0"] * 1.1
                    ax3.set_ylim(y_min, y_max)
                    st.pyplot(fig3)
                    
            else:
                with st.expander(f"âŒ {res['filename']} åˆ†æå¤±è´¥", expanded=False):
                    st.error(res["error"])

if __name__ == "__main__":
    main()